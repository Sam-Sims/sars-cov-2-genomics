---
pagetitle: "SARS-CoV-2 Genomics"
---

# Consensus Assembly

::: highlight

**Questions**

- What are the steps involved in assembling SARS-CoV-2 genome from amplicon sequencing (Illumina and Nanopore)?
- How can I do reference-based assembly of SARS-CoV-2 genomes?

**Learning Objectives**

- Summarise the steps in the bioinformatic pipeline used for reference-based assembly of SARS-CoV-2 genomes from high-throughput amplicon sequencing.
- Recognise the differences between Illumina and Nanopore pipelines.
- Apply the `connor-lab/ncov2019-artic-nf` _Nextflow_ pipeline to generate a consensus sequence from Illumina and Nanopore data.
- Troubleshoot issues when running a _Nextflow_ pipeline and resume the pipeline in case of a failure.
- Check basic data quality metrics on the assembled sequences and identify sequences for downstream analyses.

:::


## Bioinformatic Workflows/Pipelines

Bioinformatic analysis often involve multiple steps where data is gathered, cleaned and integrated to give a final set of processed files of interest to the user. 
These sequences of steps are called a _workflow_ or _pipeline_ (we will see an example in the next section). 
As analyses become more complex, workflows may include the use of many different software tools, each requiring a specific set of inputs and options to be defined. 
Furthermore, as we want to chain multiple tools together, the inputs of one tool may be the output of another, which can become challenging to manage. 

Although it is possible to code such workflows using plain _shell_ scripts, these often don't scale well across different users and compute setups. 
To overcome these limiations, dedicated [_workflow management software_](https://en.wikipedia.org/wiki/Workflow_management_system) packages have been developed to help standardise workflows and make it easier for the user to process their data. 

Two of the most popular _workflow software_ packages are [_Snakemake_](https://snakemake.readthedocs.io/en/stable/) and [_Nextflow_](https://www.nextflow.io/). 
We will not cover how to develop workflows with these packages, but rather how to use an existing workflow to generate consensus sequences from SARS-CoV-2 data.

### Why Use a Standardised Workflow?

These are some of the key advantages of using a standardised workflow for our analysis:

- Fewer errors - because the workflow automates the process of managing input/output files, there are less chances for errors or bugs in the code to occur.
- Consistency and reproducibility - analysis ran by different people should result in the same output, regardless of their computational setup.
- Software installation - all software dependencies are automatically installed for the user using solutions such as _Conda_, _Docker_ and _Singularity_ (more about these in a later section of the course).
- Scalability - workflows can run on a local desktop or scale up to run on _high performance compute clusters_.
- Checkpoint and resume - if a workflow fails in one of the tasks, it can be resumed at a later time.


## SARS-CoV-2 Workflow {.tabset}

The _Nextflow_ workflow we will use was developed by the [O'Connor Lab](https://github.com/connor-lab/ncov2019-artic-nf) and its objective was to harmonise the consensus assembly of SARS-CoV-2 genomes from both Illumina and Nanopore sequencing data.
This workflow therefore includes different sub-workflows, which are launched depending on the type of sequence data we have.

To see all the options available with this workflow, we run: 

```console
$ nextflow run ncov2019-artic-nf --help
```

```
Usage:
  nextflow run connor-lab/ncov2019-artic-nf -profile (singularity,docker,conda) ( --illumina | --nanpolish | --medaka ) --prefix [prefix] [workflow-options]

Description:
  Turn Nanopore or Illumina SARS-CoV2 sequencing reads generated by ARTIC tiling amplification protocols into consensus sequences.
    - Nanopore: ARTIC (https://github.com/artic-network/fieldbioinformatics)
    - Illumina: iVar (https://github.com/andersen-lab/ivar)

  All options set via CLI can be set in conf directory

Nextflow arguments (single DASH):
  -profile                  Allowed values: conda, singularity, docker, [COG-UK institutional profile]

Mandatory workflow arguments (mutually exclusive):
  --illumina                Run the Illumina workflow
  --nanopolish              Run the Nanopore/Nanopolish workflow (https://github.com/jts/nanopolish)
  --medaka                  Run the Nanopore/Medaka workflow (https://github.com/nanoporetech/medaka)

[...further output omitted...]
```

From the help output printed above, we can see that there are three mandatory workflow arguments named `--illumina`, `--nanopolish` and `--medaka`. 
These are the options used to launch each of the sub-workflows available to us, which we detail below.

![Add figure with workflow overview]()

Regardless of the sub-workflow used, generally speaking the main steps involved in processing the data for a given sample are (see Figure):

- Filter high-quality sequencing reads.
- Map the reads to the _Wuhan-Hu-1_ reference genome.
- Trim the primers from the aligned reads based on the primer location file (BED file).
- Perform variant calling (SNPs and indels) to identify changes relative to the reference sequence.
- Generate a consensus sequence for the sample based on those variants.
- "Mask" low-quality positions to avoid mis-identifying variants.


:::note
The _Wuhan-Hu-1_ reference genome sequence and the amplicon primer locations (in BED file format) can all be found on the ARTIC [Primer Schemes repository](https://github.com/artic-network/primer-schemes/tree/master/nCoV-2019). 
The workflow we will use takes care of downloading these files for us automatically, however it can be useful to know where to find them in case you want to use other tools that require these files. 
:::


### Illumina (FASTQ)

The Illumina sub-workflow is based on several standard bioinformatic tools and, importantly, on the [iVar](https://andersen-lab.github.io/ivar/html/) software, which was developed for analysing amplicon-based sequencing data.

![Schematic of the key steps in the `--illumina` sub-workflow.](images/workflow_illumina.svg)

In summary, the steps specific to this sub-workflow are:

- Adapter trimming with [`trim_galore`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/).
- Trimmed sequences are mapped to the _Wuhan-Hu-1_ reference genome using `bwa mem`.
  - The reference genome is "indexed" to be ready for mapping using `bwa index`.
  - The mapped files are sorted by coordinate (using `samtools sort`).
  - Unmapped reads are removed using `samtools view -F4`.
- Primers are removed from the aligned reads using `ivar trim` (using the primer BED file).
  - Reads are retained if they are at least 30bp after clipping the primers.
  - Reads are also clipped if they fall below a Phred-quality score of 20.
  - These defaults (30bp minimum length and Phred-score threshold) can be adjusted with the `--illuminaKeepLen` and `--illuminaQualThreshold` options on the nextflow pipeline. Keeping the default options is advised.
  - There is an option to retain reads even if no primer is detected on them (`--allowNoprimer`). This should be set depending on the protocol used: ligation = false, tagmentation = true (default: true).
- Variant calling is done using `ivar variants`.
  - Only positions with at least 10x depth are called. Generally it is advised not to go below this threshold.
- Consensus sequences are called using `ivar consensus`. 
  - Only positions with a minimum depth of 10x are called. Otherwise, they are marked as an ambiguous base 'N'. 

To run our pipeline on Illumina data, we use the following command:

```console
nextflow run ncov2019-artic-nf \
  -with-report -with-dag \
  -profile conda \
  --outdir results/consensus/ \
  --prefix run_name \
  --schemeVersion V3 \
  --directory data/sequences/ \
  --illumina
```

The key option here is `--illumina`, which makes sure that the correct sub-workflow will be used. 

### Nanopore (basecalled FASTQ)

The nanopore sub-workflow is based on the [ARTIC bioinformatics protocol](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html) and uses several of the tools from the accompanying [`artic` software package](https://artic.readthedocs.io/en/latest/). 

This sub-workflow is similar to the other nanopore sub-workflow, the main difference is the software used for generating a consensus sequence (`medata` instead of `nanopolish`).

![Schematic of the key steps in the `--medaka` sub-workflow.](images/workflow_medaka.svg)

In summary, the steps involved in this sub-workflow are:

- Agregate and filter reads to ensure they pass minimum read length thresholds using `artic guppyplex`:
  - minimum length 400bp (can be changed with `--min_length` option)
  - maximum length 700bp (can be changed with `--max_length` option)
- Run the `artic minion` tool, which internally does:
  - Read mapping to reference genome using `minimap2` (can be changed to use `bwa mem` with the `--bwa` option).
  - Trim primers from the aligned reads based on the known primer positions in the BED file (using a custom python script called `align_trim.py`).
  - Call consensus sequences and variants using `medaka consensus` and `medaka variant`:
  - Positions with less than 20x depth are "masked" by assigning the ambiguous base 'N'. It is not advised to go below this threshold as the models used to call variants do not perform as well.

To run our pipeline on basecalled data (FASTQ files), we use the following command:

```console
nextflow run ncov2019-artic-nf \
  -with-report -with-dag \
  -profile conda \
  --outdir results/consensus/ \
  --prefix run_name \
  --schemeVersion V3 \
  --basecalled_fastq data/sequences/ \
  --medaka
```

The key option here is `--medaka`, which makes sure that the correct sub-workflow will be used. 
We also specify the directory containing our basecalled FASTQ files with `--basecalled_fastq`.
This should contain sub-directories for each barcoded sample following the naming convention `barcodeXXXX` (where X is a number between 0 and 9). 
By default, `guppy_basecaller` generates such a folder structure. 

:::warning
For this workflow to work, you should not use compressed FASTQ files (`fastq.gz`) but rather uncompressed files. 
Otherwise the workflow will fail with an error "Couldn't detect whether your Nanopore run was barcoded or not."
:::

### Nanopore (signal-level FAST5)

The nanopore sub-workflow is based on the [ARTIC bioinformatics protocol](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html) and uses several of the tools from the accompanying [`artic` software package](https://artic.readthedocs.io/en/latest/). 

This sub-workflow is similar to the other nanopore sub-workflow, the main difference is the software used for generating a consensus sequence (`nanopolish` instead of `medaka`).

![Schematic of the key steps in the `--nanopolish` sub-workflow.](images/workflow_nanopolish.svg)

In summary, the steps involved in this sub-workflow are:

- Filter reads to ensure they pass minimum read length thresholds:
  - minimum length 400bp (can be changed with `--min_length` option)
  - maximum length 700bp (can be changed with `--max_length` option)
- Run the `artic minion` tool, which internally does:
  - Read alignment to reference genome using `minimap2` (can be changed to use `bwa mem` with the `--bwa` option).
  - Trim primers from the aligned reads (based on the known primer positions in the BED file).
  - Call consensus sequences and variants using `nanopolish variants` if using signal-level FAST5 files.
    - Positions with less than 20x depth are assigned the ambiguous base 'N'. It is not advised to go below this threshold as the models used to call variants do not perform as well.
- Unmapped reads are removed using `samtools view -F4`.

To run our pipeline on signal-level data (FAST5 files), we use the following command: 

```console
nextflow run ncov2019-artic-nf \
  -with-report -with-dag \
  -profile conda \
  --outdir results/consensus/ \
  --prefix run_name \
  --schemeVersion V3 \
  --fast5_pass data/sequences/ \
  --sequencing_summary data/sequences/sequencing_summary.txt \
  --nanopolish
```

The key option here is `--nanopolish`, which makes sure the correct sub-workflow is used.
We also need to specify a directory containing our FAST5 files with `--fast5_pass` and a path to the `sequencing_summary.txt` file that is standard output from the `guppy` software.

## {.unlisted .unnumbered}

The first few arguments used in the above commands are generic options for Nextflow (they can be used with any workflow, not just specifically with our SARS-CoV-2 workflow):

- `-with-report` generates a report about the resources used at different steps in the pipeline (e.g. how long each step took to run, how many CPUs and memory was used, etc.). These will be found in a sub-directory called `pipeline_info`.
- `-with-dag` produces a "directed acyclic graph", which shows how the different steps of the pipeline link to each other. This will be saved in the same `pipeline_info` directory.
- `-profile conda` uses the _Conda_ package manager to automatically install all the necessary software used by this pipeline (we will talk more about _Conda_ later in the course).

The following arguments are specific to our workflow:

- `--illumina`, `--medaka` or `--nanopolish` indicates whether want to use the pipeline steps developed for Illumina or Nanopore data.
- `--outdir results/consensus` indicates that we want the results of the pipeline to be saved in the directory `results/consensus` (the directory will be created if it does not exist).
- `--prefix run_name` is a prefix that Nextflow will use to name some of the output files. This is useful if we had multiple runs in the same project and wanted to run the pipeline on each run. 
- `--schemeVersion V3` indicates the Artic primer version used when preparing the sequencing libraries (V1, V2, V3 or V4). 
- `--directory`, `--basecalled_fastq` or `--fast5_pass`  is the directory where all the sequencing data is stored. For Illumina, the workflow will automatically recognise files with suffix "_1.fastq.gz" or "_2.fastq.gz" as being paired-end data.


:::note
The term **masking** is often used to refer to the process of converting sequence bases to the ambiguous character 'N'. 
You may come across this term in the documentation of certain tools, for example: 
"Positions with less than 20x depth of sequencing are masked."

Masks are not limited to depth of sequencing. For example, [reference genomes from ENSEMBL](https://ensemblgenomes.org/) are available with masked repeat or low-complexity sequences (e.g. around centromeres, transposon-rich regions, etc.). 

The term **soft masking** is also used to refer to cases where, instead of using the ambiguous character 'N', sequences are masked with a lowercase. 
For example:

```
>seq_with_soft_masking
ACAGACTGACGCTGTcatgtatgtcgacGATAGGCTGATGGCGAGTGACTCGAG
>seq_with_hard_masking
ACAGACTGACGCTGTNNNNNNNNNNNNNGATAGGCTGATGGCGAGTGACTCGAG
```
:::


## Output Files {.tabset}

After running our pipeline, we will get several output directories. 
Again, the directories we get will depend on which version of the workflow was used. 

### Illumina

The output directory will contain the following folders:

| Directory | Description |
| :- | :- |
| `ncovIllumina_sequenceAnalysis_readTrimming` | FASTQ files with Illumina adapters trimmed. | 
| `ncovIllumina_sequenceAnalysis_readMapping` | BAM files with reads mapped to the reference genome. |
| `ncovIllumina_sequenceAnalysis_trimPrimerSequences` | same BAM files but with primers trimmed from the reads. |
| `ncovIllumina_sequenceAnalysis_makeConsensus` | FASTA files with consensus sequences for each sample. |
| `ncovIllumina_sequenceAnalysis_callVariants` | TSV files with variants identified in each sample. |
| `qc_plots` | PNG files with coverage plots (detailed below). |
| `qc_pass_climb_upload` | FASTA and BAM files for samples passing default QC thresholds. |

We also get a file in the output directory that compiles several quality metrics called `PREFIX.qc.csv` (where "PREFIX" is what was defined with the `--prefix` option when running the workflow).


### Nanopore

| Directory | Description |
| :- | :- |
| `articNcovNanopore_sequenceAnalysisMedaka_articGuppyPlex` | FASTQ files with filtered reads. | 
| `articNcovNanopore_sequenceAnalysisMedaka_articMinIONMedaka` | Output files from the "medaka" sub-workflow (see next table). |
| `qc_plots` | PNG files with coverage plots (detailed below). |
| `qc_pass_climb_upload` | FASTA and BAM files for samples passing default QC thresholds. |

The "medaka" sub-workflow outputs several files for each sample. 
Some of the more relevant ones being:

| File Name | Description |
| :- | :- |
| `*.consensus.fasta` | Consensus FASTA file. |
| `*.pass.vcf.gz` | VCF file with identified variants (relative to the Wuhan reference) that pass quality thresholds. |
| `*.primertrimmed.rg.sorted.bam` | BAM file with mapped reads (with primers trimmed). |

Finally, we also get a file in the output directory that compiles several quality metrics called `PREFIX.qc.csv` (where "PREFIX" is what was defined with the `--prefix` option when running the workflow).

## {.unlisted .unnumbered}


## Quality Control 

The first file we want to look at in terms of quality control is the CSV in our workflow output directory.
This file can be open in any spreadsheet software (e.g. Excel or LibreOffice). 

For each sequence, it contains information about: 

- `sample_name` The name of the sample.
- `pct_N_bases` The percentage of bases in the consensus sequence that were marked as ambiguous ('N'). These are bases that didn't pass the minimum sequencing depth threshold (10x for Illumina or 20x for Nanopore).
- `pct_covered_bases` The percentage of bases that were called (i.e. above the sequencing depth threshold).
- `longest_no_N_run` The maximum length of consecutive ambiguous 'N' bases in the consensus sequence.
- `num_aligned_reads` Total number of reads aligned to the reference genome.
- `fasta` Name of the FASTA file.
- `bam` Name of the BAM file.
- `qc_pass` Contains TRUE or FALSE depending on whether the sample passed default quality thresholds.

The **default quality thresholds are very permissive** (and unfortunately cannot be changed). 
A sample "passes" as high quality if at least 50% of the genome is covered. 
This is not a very high threshold, and typical recommendations are to only use samples in downstream analysis if at they have at least 90% coverage. 

Therefore, it is always good to open this file and sort the samples by each of these quality metrics to identify any problematic samples (and exclude them from downstream analysis). 

Another helpful quality control output are the plots found in the `qc_plots` folder.

![Quality control plots showing: the average read depth across 200bp windows (green line); the fraction of 'N' ambiguous bases across 10bp windows (red).](images/consensus_qc.svg)


For the nanopore sub-workflow you also get coverage barplots for individual primer pairs, which is useful to identify any particular fragments that did not get amplified.

![Quality control barplot for nanopore data showing the read depth for each individual PCR amplicon. Blue and orange show the two pools of primers used for generating the libraries.](images/barcode_qc.svg)

These may be particularly useful to identify if there is a systematic "dropout" of particular amplicons. 
This may occur, for example, if a common variant has mutations that affect the primer hybridization during PCR. 

:::exercise

**This is an extended exercise, which uses skills from all topics covered up to this point.**

In the `day2/example1` directory of our course materials, you will find data for a series of samples that were sequenced to investigate the prevalence of different SARS-CoV-2 variants in the UK and India between Jan and June 2021. 

Make sure you change to that directory first: `cd ~/Course_Materials/day2/example1/`.
The data (and metadata) can be be found in the `data` folder. 

----

**UK samples**

The UK samples were sequenced on an Illumina NovaSeq 6000 sequencer, to produce 300bp paired-end reads (the data given has already been pre-processed to remove illumina adapters and so the reads are in reality shorter at around 250bp).

- Using command-line tools determine how many samples we have data for.
- Write a shell script that:
  - Runs FastQC through all the samples. Output the results in a directory called `results/fastqc/`.
  - Compile the results of FastQC using MultiQC and output its results in a directory called `results/multiqc/`.
  - Analyse the reports and make a note of any samples that you worry about in terms of producing a high-quality assembly.
- Run all the samples through the `ncov2019-artic-nf` Nextflow pipeline. (Note: this will take a around 2h to run.)
  - Output the results in a directory called `results/consensus_uk/`.
  - Analyse the QC CSV file produced by the pipeline and check how many samples pass quality thresholds. How does this compare with your assessment of the raw data quality from the previous step? Can you relate this to the information present in the metadata CSV file?

Save your scripts in a directory called `scripts/`.

----

**India samples**

These samples were sequenced on an Nanopore MinION sequencer. 

- Run all the samples through the `ncov2019-artic-nf` Nextflow pipeline. (Note: this will take a around 2h to run.)
  - Output the results to a directory called `results/consensus_india/`.
  - Analyse the QC CSV file produced by the pipeline and check how many samples pass quality thresholds. 
  - Do you agree that all these samples should be used in downstream analysis? How many samples would you keep for uploading to public databases and lineage assignment?

<!--
**NOTE**
At the moment one of the samples (SRR14493633) is making the pipeline fail, throwing an error with bcftools consensus. 
I think the problem is that the "preconsensus" sequence contains an 'N', whereas the VCF still contains the reference genome base. 
This may be fixed in later versions of `artic`, but unfortunately the connor-lab workflow is still using an older version. 

I excluded that sample and things seem to work fine for all other samples. 
-->


<details><summary>Answer</summary>

To check how many samples we have, we can combine the `ls` (list files) and `wc` (word count) commands:

```console
$ ls data/uk_illumina/*_1.fastq.gz | wc -l
```

Because we have paired-end data (two files per sample) we list all files that end with "_1.fastq.gz".
This way we only count each sample once. 
We then pass the output of the `ls` command to the `wc` command with the option `-l` to count the lines of text coming as output from the previous command. 

----

To check the data quality of our reads, we can write a shell script including the commands we want to run:

```bash
#!/bin/bash

# make directory
mkdir -p results/fastqc
mkdir -p results/multiqc

# run FastQC on all the files
# using 8 threads in parallel since we have 8 CPUs available
fastqc -t 8 --outdir results/fastqc/ data/run1/*.fastq.gz

# run MultiQC on the output of FastQC
multiqc --outdir results/multiqc/ results/fastqc/
```

This script starts by having some code to create the necessary output directories. 
The `-p` option ensures that we don't get an error in case the directory already exists. 

We then run `fastqc`, being careful to specify that we have 8 CPUs (with the `-t` option). 
We also use the `*` _wildcard_ to pattern-match all the files ending with the file extension ".fastq.gz", so that FastQC will automatically process all the files in parallel. 

We then use the _output_ directory of the `fastqc` step as the _input_ for `multiqc`. 
This is just like a mini-workflow or our own! Outputs of one tool feeding into the next tool.

After looking at the quality report from `multiqc`, we can notice that some of the samples seem to have quite a low number of sequences. 
Looking at the metadata table, we can see that these have quite a high Ct number (from qPCR done prior to library preparation). 
We should keep an eye on these samples and check if they pass the QC thresholds from our consensus assembly workflow.

----

To produce consensus sequences, we run our nextflow workflow with the `--illumina` option and `--directory` to specify the directory where all our files are located in:

```bash
nextflow run ncov2019-artic-nf \
  -with-report -with-dag \
  -profile conda \
  --outdir results/consensus_uk/ \
  --prefix uk \
  --schemeVersion V3 \
  --directory data/uk_illumina/ \
  --illumina
```

This step takes quite a while to complete (around 2h on an 8 CPU machine), but we can see its progress printed on the console while it runs. 
Once it completes, we should get several directories within `/results/consensus_uk/`. 
We also get a file named `uk.qc.csv`, which contains all the QC metrics from our assembled consensus sequences. 

If we open that file, we can see several quality metrics, which we detailed in the materials above. 

As a bonus, to find out how many samples pass quality thresholds, we could use the `grep` command (used to find text patterns within files) combined with `wc`:

```console 
$ grep "TRUE" results/consensus_uk/uk.qc.csv | wc -l
```

----

To process the Indian samples, we would instead run our workflow with `--medaka` and the `--basecalled_fastq` option to specify the directory where all our FASTQ files are located in:

```bash
nextflow run ncov2019-artic-nf \
  -with-report -with-dag \
  -profile conda \
  --outdir results/consensus_india/ \
  --prefix india \
  --schemeVersion V3 \
  --basecalled_fastq data/india_nanopore/ \
  --medaka
```

The workflow should take around 30m to run, and will produce several directories as detailed in the materials. 
Again, we can explore the QC file to identify any problematic samples. 

</details>

:::


## Summary

:::highlight

**Key Points**

- _Nextflow_ is a software used for building workflows/pipelines involving multiple tools and data processing steps. Using established workflows helps with automation, reproducibility, consistency of results and reduces the chances of data processing errors.
- The `connor-lab/ncov2019-artic-nf` workflow implements the steps to generate SARS-CoV-2 consensus sequences from _Illumina_ or _Nanopore_ data.
- The main steps to generate consensus sequences are: filter high-quality reads, map reads to reference, trim amplicon primers, variant calling, produce consensus sequence, mask low-quality positions.
- The command `nextflow run ncov2019-artic-nf` is used to run the workflow, using specific options depending on the data we have:
  - `--illumina` for Illumina FASTQ files.
  - `--medaka` for Nanopore basecalled FASTQ files.
  - `--nanopolish` for Nanopore signal-level FAST5 files.
- The output of the workflow includes several quality metrics for each sample, including genome coverage and number of ambiguous 'N' bases.
  - Sequences with > 90% coverage are recommended for downstream analysis.

:::
